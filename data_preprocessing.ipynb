{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rowpep/cross-domain-hatespeech/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C12vMkyjvnsU"
      },
      "source": [
        "DATA PREPROCESSING\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This code downloads datasets from the internet and formats them into Pandas\n",
        "\n",
        "*   This code downloads datasets from the internet and formats them into Pandas DataFrames with columns: Class - Text - Source - PostID\n",
        "*   Then the classes are made binary, where hate speech = 1 and non-hate = 0\n",
        "*   Few-shot data sets are created.\n",
        "*   Raw datasets are saved for BERT input\n",
        "*   All texts are tokenised with spacy and then saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HVW-fhLEKht"
      },
      "outputs": [],
      "source": [
        "#LIBRARIES\n",
        "\n",
        "import pandas as pd\n",
        "import urllib\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Downloading datasets and standardising the formats\n",
        "\n"
      ],
      "metadata": {
        "id": "oGgLG5AkTCZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW2CRSXKEP4J"
      },
      "outputs": [],
      "source": [
        "#DAVIDSON ET AL\n",
        "\n",
        "davidson_et_al_url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/refs/heads/master/data/labeled_data.csv\"\n",
        "\n",
        "raw_davidson_et_al_df = pd.read_csv(davidson_et_al_url, index_col=0)\n",
        "\n",
        "davidson_et_al = raw_davidson_et_al_df.drop(columns=['count', 'hate_speech', 'offensive_language', 'neither'])\n",
        "\n",
        "davidson_et_al[\"Source\"] = 'Davidson et al'\n",
        "davidson_et_al[\"Post_ID\"] = davidson_et_al.index.astype(str)\n",
        "\n",
        "davidson_et_al = davidson_et_al.rename(columns ={\"tweet\": \"Text\", \"class\": \"Class\"})\n",
        "\n",
        "davidson_dict = davidson_et_al.to_dict(orient='dict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YtQaO1Fsu5d"
      },
      "outputs": [],
      "source": [
        "#HATEXPLAIN\n",
        "\n",
        "hatexplain_url = \"https://raw.githubusercontent.com/hate-alert/HateXplain/refs/heads/master/Data/dataset.json\"\n",
        "\n",
        "raw_hatexplain_df = pd.read_json(hatexplain_url)\n",
        "hatexplain_df = raw_hatexplain_df.transpose().drop(columns=['rationales'])\n",
        "hatexplain_df = hatexplain_df[hatexplain_df[\"post_id\"].str.endswith(\"twitter\").copy()]\n",
        "\n",
        "def majority_label(annotations):\n",
        "  labels = [ann['label'] for ann in annotations]\n",
        "  return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "hatexplain_df['majority_label'] = hatexplain_df['annotators'].apply(majority_label)\n",
        "\n",
        "hatexplain_df = hatexplain_df.drop(columns=['annotators'])\n",
        "\n",
        "hatexplain_df[\"Source\"] = 'HateXplain'\n",
        "\n",
        "hatexplain_df['post_tokens'] = hatexplain_df['post_tokens'].apply(lambda tokens:' '.join(tokens))\n",
        "\n",
        "hatexplain_df = hatexplain_df.rename(columns ={\"majority_label\": \"Class\", \"post_tokens\": \"Text\", \"post_id\": \"Post_ID\"})\n",
        "\n",
        "hatexplain_df = hatexplain_df[['Class','Text','Source','Post_ID']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ4rOhaM0PBU"
      },
      "outputs": [],
      "source": [
        "#WIKIPEDIA DETOX\n",
        "#code taken from: https://github.com/ewulczyn/wiki-detox/blob/master/src/figshare/Wikipedia%20Talk%20Data%20-%20Getting%20Started.ipynb\n",
        "\n",
        "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634'\n",
        "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637'\n",
        "\n",
        "\n",
        "def download_file(url, fname):\n",
        "    urllib.request.urlretrieve(url, fname)\n",
        "\n",
        "\n",
        "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
        "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')\n",
        "\n",
        "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
        "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')\n",
        "\n",
        "# labels a comment as an atack if the majority of annoatators did so\n",
        "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
        "\n",
        "# join labels and comments\n",
        "comments['attack'] = labels\n",
        "\n",
        "# remove newline and tab tokens\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
        "\n",
        "\n",
        "raw_wikipedia_df = comments\n",
        "\n",
        "wikipedia_df = raw_wikipedia_df.drop(columns = ['year', 'logged_in', 'sample', 'ns', 'split'])\n",
        "\n",
        "wikipedia_df[\"Source\"] = 'Wikipedia'\n",
        "wikipedia_df[\"Post_ID\"] = wikipedia_df.index.astype(str)\n",
        "\n",
        "wikipedia_df = wikipedia_df.rename(columns = {\"attack\": \"Class\", \"comment\": \"Text\"})\n",
        "\n",
        "wikipedia_df = wikipedia_df[['Class','Text','Source','Post_ID']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_VvL6Tt5aQP"
      },
      "outputs": [],
      "source": [
        "#REDDIT SLUR CORPUS\n",
        "\n",
        "reddit_url = 'https://raw.githubusercontent.com/networkdynamics/slur-corpus/refs/heads/main/kurrek.2020.slur-corpus.csv'\n",
        "\n",
        "raw_reddit_df = pd.read_csv(reddit_url)\n",
        "\n",
        "reddit_df = raw_reddit_df.drop(columns = ['link_id', 'parent_id', 'score', 'subreddit', 'slur', 'disagreement', 'author'])\n",
        "\n",
        "reddit_df[\"Source\"] = 'Reddit'\n",
        "\n",
        "reddit_df = reddit_df.rename(columns = {\"gold_label\": \"Class\", \"body\": \"Text\", \"id\": \"Post_ID\"})\n",
        "\n",
        "reddit_df = reddit_df[['Class','Text','Source','Post_ID']]\n",
        "\n",
        "#only keep rows where the text is a string as some of them are None\n",
        "reddit_df = reddit_df[reddit_df['Text'].apply(lambda x: isinstance(x, str))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkD4MrXl06E6"
      },
      "outputs": [],
      "source": [
        "#GAB HATE CORPUS\n",
        "\n",
        "!pip install osfclient\n",
        "!osf -p edua3 list\n",
        "!osf -p edua3 fetch osfstorage/Data/GabHateCorpus_annotations.tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbyJ7v6BU5dh"
      },
      "outputs": [],
      "source": [
        "raw_gab_df = pd.read_csv(\"GabHateCorpus_annotations.tsv\", sep=\"\\t\")\n",
        "\n",
        "gab_df = raw_gab_df.drop(columns = ['HD', 'CV', 'VO', 'REL', 'RAE', 'SXO', 'GEN', 'IDL', 'NAT', 'POL', 'MPH', 'EX', 'IM'])\n",
        "\n",
        "\n",
        "#Ties are broken toward the positive class (i.e., if there’s no clear majority, the post is labeled as hate speech).\n",
        "#This means:\n",
        "#If annotators are split 1 hate / 1 offensive / 1 normal = it’s treated as hateful.\n",
        "#If 1 hate / 2 normal = labeled normal.\n",
        "#If 2 hate / 1 normal = labeled hate.\n",
        "\n",
        "def majority_vote(hate_labels):\n",
        "  counts = hate_labels.value_counts()\n",
        "  if counts.get(1, 0) >= counts.get(0,0):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "gab_df['majority_vote'] = gab_df.groupby('ID')['Hate'].transform(majority_vote)\n",
        "\n",
        "gab_df = gab_df.drop_duplicates(subset='ID').drop(columns = ['Annotator','Hate'])\n",
        "\n",
        "gab_df[\"Source\"] = 'Gab'\n",
        "gab_df = gab_df.rename(columns = {\"ID\": \"Post_ID\", \"majority_vote\": \"Class\"})\n",
        "\n",
        "gab_df = gab_df[['Class','Text','Source','Post_ID']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THE DATASETS\n",
        "\n",
        "# davidson_et_al\n",
        "# hatexplain_df\n",
        "# wikipedia_df\n",
        "# reddit_df\n",
        "# gab_df"
      ],
      "metadata": {
        "id": "xXTT4lwfUI9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00QYCxqlhIFG"
      },
      "source": [
        "\n",
        "Making the classes binary. Hate speech = 1 and non-hate = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYAlKlXthH1m"
      },
      "outputs": [],
      "source": [
        "#Davidson et al\n",
        "#0 = hatespeech, 1 = offensive language, 2 = neither\n",
        "\n",
        "bin_davidson_et_al = davidson_et_al.copy()\n",
        "bin_davidson_et_al['Binary_Class'] = bin_davidson_et_al['Class'].map({0:1, 1:0, 2:0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phjZOQkajV7O"
      },
      "outputs": [],
      "source": [
        "#HateXplain\n",
        "\n",
        "bin_hatexplain_df = hatexplain_df.copy()\n",
        "bin_hatexplain_df['Binary_Class'] = bin_hatexplain_df['Class'].map({\"normal\": 0, \"offensive\": 0, \"hatespeech\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo3oXPf8j9CJ"
      },
      "outputs": [],
      "source": [
        "#Wikipedia\n",
        "\n",
        "bin_wikipedia_df = wikipedia_df.copy()\n",
        "bin_wikipedia_df['Binary_Class'] = bin_wikipedia_df['Class'].map({False: 0, True: 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqCpYb3Nk_c8"
      },
      "outputs": [],
      "source": [
        "#Reddit\n",
        "\n",
        "bin_reddit_df = reddit_df.copy()\n",
        "\n",
        "bin_reddit_df = bin_reddit_df.dropna(subset=['Class'])\n",
        "\n",
        "bin_reddit_df['Binary_Class'] = bin_reddit_df['Class'].map({\"DEG\": 1, \"NDG\": 0, \"HOM\": 0, \"APR\": 0, \"CMP\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4YejZ9HmbW3"
      },
      "outputs": [],
      "source": [
        "#Gab\n",
        "\n",
        "bin_gab_df = gab_df.copy()\n",
        "bin_gab_df['Binary_Class'] = bin_gab_df['Class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-10GszjpbAaU"
      },
      "outputs": [],
      "source": [
        "#BINARY DATASETS\n",
        "\n",
        "# bin_davidson_et_al\n",
        "# bin_hatexplain_df\n",
        "# bin_wikipedia_df\n",
        "# bin_reddit_df\n",
        "# bin_gab_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d21bv3xa6A3"
      },
      "source": [
        "Creating text lists of classes and tweets for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFCawiGKa8CE"
      },
      "outputs": [],
      "source": [
        "davidson_text_list = bin_davidson_et_al['Text'].tolist()\n",
        "davidson_class_list = bin_davidson_et_al['Binary_Class'].tolist()\n",
        "\n",
        "hatexplain_text_list = bin_hatexplain_df['Text'].tolist()\n",
        "hatexplain_class_list = bin_hatexplain_df['Binary_Class'].tolist()\n",
        "\n",
        "wikipedia_text_list = bin_wikipedia_df['Text'].tolist()\n",
        "wikipedia_class_list = bin_wikipedia_df['Binary_Class'].tolist()\n",
        "\n",
        "reddit_text_list = bin_reddit_df['Text'].tolist()\n",
        "reddit_class_list = bin_reddit_df['Binary_Class'].tolist()\n",
        "\n",
        "gab_text_list = bin_gab_df['Text'].tolist()\n",
        "gab_class_list = bin_gab_df['Binary_Class'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odH5gKi4_mqI"
      },
      "source": [
        "Creating few-shot samples and new datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Cyak6k_oZ6"
      },
      "outputs": [],
      "source": [
        "#Reddit few shot\n",
        "\n",
        "#creating few-shot samples from Reddit\n",
        "reddit_fewshot_text, fs_reddit_text_list, reddit_fewshot_label, fs_reddit_class_list = train_test_split(reddit_text_list, reddit_class_list, test_size=0.90, stratify=reddit_class_list, random_state=42)\n",
        "\n",
        "#Davidson + Reddit\n",
        "davidson_fs_reddit_text = reddit_fewshot_text + davidson_text_list\n",
        "davidson_fs_reddit_labels = reddit_fewshot_label + davidson_class_list\n",
        "\n",
        "#Hatexplain + Reddit\n",
        "hatexplain_fs_reddit_text = reddit_fewshot_text + hatexplain_text_list\n",
        "hatexplain_fs_reddit_labels = reddit_fewshot_label + hatexplain_class_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD1S_yv0lKSq"
      },
      "outputs": [],
      "source": [
        "#Gab few shot\n",
        "\n",
        "#creating few-shot samples from Gab\n",
        "gab_fewshot_text, fs_gab_text_list, gab_fewshot_label, fs_gab_class_list = train_test_split(gab_text_list, gab_class_list, test_size=0.90, stratify=gab_class_list, random_state=42)\n",
        "\n",
        "#Davidson + Gab\n",
        "davidson_fs_gab_text = gab_fewshot_text + davidson_text_list\n",
        "davidson_fs_gab_labels = gab_fewshot_label + davidson_class_list\n",
        "\n",
        "#HateXplain + Gab\n",
        "hatexplain_fs_gab_text = gab_fewshot_text + hatexplain_text_list\n",
        "hatexplain_fs_gab_labels = gab_fewshot_label + hatexplain_class_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofeL7srsIJ2"
      },
      "outputs": [],
      "source": [
        "#Wikipedia few shot\n",
        "\n",
        "#creating few-shot samples from Wikipedia\n",
        "wikipedia_fewshot_text, fs_wikipedia_text_list, wikipedia_fewshot_label, fs_wikipedia_class_list = train_test_split(wikipedia_text_list, wikipedia_class_list, test_size=0.90, stratify=wikipedia_class_list, random_state=42)\n",
        "\n",
        "#Davidson + Wikipedia\n",
        "davidson_fs_wikipedia_text = wikipedia_fewshot_text + davidson_text_list\n",
        "davidson_fs_wikipedia_labels = wikipedia_fewshot_label + davidson_class_list\n",
        "\n",
        "#Hatexplain + Wikipedia\n",
        "hatexplain_fs_wikipedia_text = wikipedia_fewshot_text + hatexplain_text_list\n",
        "hatexplain_fs_wikipedia_labels = wikipedia_fewshot_label + hatexplain_class_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5FAS5vWsjyG"
      },
      "outputs": [],
      "source": [
        "#ALL OF THE DATASETS\n",
        "\n",
        "# #Zero-shot sets\n",
        "\n",
        "# davidson_class_list\n",
        "# davidson_text_list\n",
        "# hatexplain_class_list\n",
        "# hatexplain_text_list\n",
        "# reddit_class_list\n",
        "# reddit_text_list\n",
        "# gab_class_list\n",
        "# gab_text_list\n",
        "# wikipedia_class_list\n",
        "# wikipedia_text_list\n",
        "\n",
        "# #Few shot sets\n",
        "\n",
        "# #Reddit sets\n",
        "\n",
        "# fs_reddit_class_list\n",
        "# fs_reddit_text_list\n",
        "# hatexplain_fs_reddit_labels\n",
        "# hatexplain_fs_reddit_text\n",
        "# davidson_fs_reddit_labels\n",
        "# davidson_fs_reddit_text\n",
        "\n",
        "# #Gab sets\n",
        "\n",
        "# fs_gab_text_list\n",
        "# fs_gab_class_list\n",
        "# hatexplain_fs_gab_labels\n",
        "# hatexplain_fs_gab_text\n",
        "# davidson_fs_gab_labels\n",
        "# davidson_fs_gab_text\n",
        "\n",
        "# #Wikipedia sets\n",
        "\n",
        "# fs_wikipedia_text_list\n",
        "# fs_wikipedia_class_list\n",
        "# hatexplain_fs_wikipedia_labels\n",
        "# hatexplain_fs_wikipedia_text\n",
        "# davidson_fs_wikipedia_labels\n",
        "# davidson_fs_wikipedia_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the raw datasets for use as BERT inputs.\n",
        "\n",
        "Save files as .json.\n",
        "\n"
      ],
      "metadata": {
        "id": "oNBmwHcSWQTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07kby6lGxOlU",
        "outputId": "a1a24b62-25e0-49da-d2e7-b1c91b2ed335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save all this data for BERT bc it needs raw data\n",
        "\n",
        "#Zero-shot sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_class_list.json','w') as f:\n",
        "  json.dump(davidson_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_text_list.json','w') as f:\n",
        "  json.dump(davidson_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_class_list.json','w') as f:\n",
        "  json.dump(hatexplain_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_text_list.json','w') as f:\n",
        "  json.dump(hatexplain_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_reddit_class_list.json','w') as f:\n",
        "  json.dump(reddit_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_reddit_text_list.json','w') as f:\n",
        "  json.dump(reddit_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_gab_class_list.json','w') as f:\n",
        "  json.dump(gab_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_gab_text_list.json','w') as f:\n",
        "  json.dump(gab_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_wikipedia_class_list.json','w') as f:\n",
        "  json.dump(wikipedia_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_wikipedia_text_list.json','w') as f:\n",
        "  json.dump(wikipedia_text_list, f)\n",
        "\n",
        "#Few shot sets\n",
        "\n",
        "#Reddit sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_reddit_class_list.json','w') as f:\n",
        "  json.dump(fs_reddit_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_reddit_text_list.json','w') as f:\n",
        "  json.dump(fs_reddit_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_reddit_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_reddit_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_reddit_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_reddit_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_reddit_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_reddit_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_reddit_text.json','w') as f:\n",
        "  json.dump(davidson_fs_reddit_text, f)\n",
        "\n",
        "#Gab sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_gab_class_list.json','w') as f:\n",
        "  json.dump(fs_gab_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_gab_text_list.json','w') as f:\n",
        "  json.dump(fs_gab_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_gab_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_gab_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_gab_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_gab_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_gab_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_gab_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_gab_text.json','w') as f:\n",
        "  json.dump(davidson_fs_gab_text, f)\n",
        "\n",
        "#Wikipedia sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_wikipedia_class_list.json','w') as f:\n",
        "  json.dump(fs_wikipedia_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_fs_wikipedia_text_list.json','w') as f:\n",
        "  json.dump(fs_wikipedia_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_wikipedia_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_wikipedia_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_hatexplain_fs_wikipedia_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_wikipedia_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_wikipedia_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_wikipedia_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/bert-input/bert_davidson_fs_wikipedia_text.json','w') as f:\n",
        "  json.dump(davidson_fs_wikipedia_text, f)"
      ],
      "metadata": {
        "id": "jW2W3v1hw-1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4g4IgJJqwD_"
      },
      "source": [
        "spaCy Tokenisation of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uant0QEcqv0X"
      },
      "outputs": [],
      "source": [
        "def cleanwithspacy(text):\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.text for token in doc\n",
        "            if not token.is_stop\n",
        "            and not token.is_punct\n",
        "            and not token.like_url\n",
        "            and token.text.strip()]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tpounphPudhv"
      },
      "outputs": [],
      "source": [
        "fs_reddit_text_list_spcy = [cleanwithspacy(text) for text in fs_reddit_text_list]\n",
        "\n",
        "hatexplain_fs_reddit_text_spcy = [cleanwithspacy(text) for text in hatexplain_fs_reddit_text]\n",
        "\n",
        "davidson_fs_reddit_text_spcy = [cleanwithspacy(text) for text in davidson_fs_reddit_text]\n",
        "\n",
        "fs_gab_text_list_spcy = [cleanwithspacy(text) for text in fs_gab_text_list]\n",
        "\n",
        "hatexplain_fs_gab_text_spcy = [cleanwithspacy(text) for text in hatexplain_fs_gab_text]\n",
        "\n",
        "davidson_fs_gab_text_spcy = [cleanwithspacy(text) for text in davidson_fs_gab_text]\n",
        "\n",
        "fs_wikipedia_text_list_spcy = [cleanwithspacy(text) for text in fs_wikipedia_text_list]\n",
        "\n",
        "hatexplain_fs_wikipedia_text_spcy = [cleanwithspacy(text) for text in hatexplain_fs_wikipedia_text]\n",
        "\n",
        "davidson_fs_wikipedia_text_spcy = [cleanwithspacy(text) for text in davidson_fs_wikipedia_text]\n",
        "\n",
        "davidson_text_list_spcy = [cleanwithspacy(text) for text in davidson_text_list]\n",
        "\n",
        "hatexplain_text_list_spcy = [cleanwithspacy(text) for text in hatexplain_text_list]\n",
        "\n",
        "reddit_text_list_spcy = [cleanwithspacy(text) for text in reddit_text_list]\n",
        "\n",
        "gab_text_list_spcy = [cleanwithspacy(text) for text in gab_text_list]\n",
        "\n",
        "wikipedia_text_list_spcy = [cleanwithspacy(text) for text in wikipedia_text_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blJbWcnCscz-"
      },
      "outputs": [],
      "source": [
        "#List of all the datasets, now tokenised with spacy\n",
        "\n",
        "# #Zero-shot sets\n",
        "\n",
        "# davidson_class_list\n",
        "# davidson_text_list_spcy\n",
        "# hatexplain_class_list\n",
        "# hatexplain_text_list_spcy\n",
        "# reddit_class_list\n",
        "# reddit_text_list_spcy\n",
        "# gab_class_list\n",
        "# gab_text_list_spcy\n",
        "# wikipedia_class_list\n",
        "# wikipedia_text_list\n",
        "\n",
        "# #Few shot sets\n",
        "\n",
        "# #Reddit sets\n",
        "\n",
        "# fs_reddit_class_list\n",
        "# fs_reddit_text_list\n",
        "# hatexplain_fs_reddit_labels\n",
        "# hatexplain_fs_reddit_text\n",
        "# davidson_fs_reddit_labels\n",
        "# davidson_fs_reddit_text\n",
        "\n",
        "# #Gab sets\n",
        "\n",
        "# fs_gab_text_list\n",
        "# fs_gab_class_list\n",
        "# hatexplain_fs_gab_labels\n",
        "# hatexplain_fs_gab_text\n",
        "# davidson_fs_gab_labels\n",
        "# davidson_fs_gab_text\n",
        "\n",
        "# #Wikipedia sets\n",
        "\n",
        "# fs_wikipedia_text_list\n",
        "# fs_wikipedia_class_list\n",
        "# hatexplain_fs_wikipedia_labels\n",
        "# hatexplain_fs_wikipedia_text\n",
        "# davidson_fs_wikipedia_labels\n",
        "# davidson_fs_wikipedia_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuj1yzKKisYP"
      },
      "source": [
        "Save the new datasets that will be used for all models except BERT.\n",
        "\n",
        "Save them as .json files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wnkJ5z2iuQZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1VWxXkfirr_"
      },
      "outputs": [],
      "source": [
        "#zero shot sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_class_list.json','w') as f:\n",
        "  json.dump(davidson_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_text_list_spcy.json','w') as f:\n",
        "  json.dump(davidson_text_list_spcy, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_class_list.json','w') as f:\n",
        "  json.dump(hatexplain_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_text_list_spcy.json','w') as f:\n",
        "  json.dump(hatexplain_text_list_spcy, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/reddit_class_list.json','w') as f:\n",
        "  json.dump(reddit_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/reddit_text_list_spcy.json','w') as f:\n",
        "  json.dump(reddit_text_list_spcy, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/gab_class_list.json','w') as f:\n",
        "  json.dump(gab_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/gab_text_list_spcy.json','w') as f:\n",
        "  json.dump(gab_text_list_spcy, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/wikipedia_class_list.json','w') as f:\n",
        "  json.dump(wikipedia_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/wikipedia_text_list_spcy.json','w') as f:\n",
        "  json.dump(wikipedia_text_list_spcy, f)\n",
        "\n",
        "#Few shot sets\n",
        "\n",
        "#Reddit sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_reddit_class_list.json','w') as f:\n",
        "  json.dump(fs_reddit_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_reddit_text_list.json','w') as f:\n",
        "  json.dump(fs_reddit_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_reddit_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_reddit_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_reddit_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_reddit_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_reddit_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_reddit_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_reddit_text.json','w') as f:\n",
        "  json.dump(davidson_fs_reddit_text, f)\n",
        "\n",
        "\n",
        "#Gab sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_gab_class_list.json','w') as f:\n",
        "  json.dump(fs_gab_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_gab_text_list.json','w') as f:\n",
        "  json.dump(fs_gab_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_gab_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_gab_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_gab_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_gab_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_gab_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_gab_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_gab_text.json','w') as f:\n",
        "  json.dump(davidson_fs_gab_text, f)\n",
        "\n",
        "\n",
        "#Wikipedia sets\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_wikipedia_class_list.json','w') as f:\n",
        "  json.dump(fs_wikipedia_class_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/fs_wikipedia_text_list.json','w') as f:\n",
        "  json.dump(fs_wikipedia_text_list, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_wikipedia_labels.json','w') as f:\n",
        "  json.dump(hatexplain_fs_wikipedia_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/hatexplain_fs_wikipedia_text.json','w') as f:\n",
        "  json.dump(hatexplain_fs_wikipedia_text, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_wikipedia_labels.json','w') as f:\n",
        "  json.dump(davidson_fs_wikipedia_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/DissData/davidson_fs_wikipedia_text.json','w') as f:\n",
        "  json.dump(davidson_fs_wikipedia_text, f)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPNcHHA97V1XFUzu51OL9EC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}